{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39e129b",
   "metadata": {},
   "source": [
    "# Comparing Original LeetCode Solutions to Online Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd5ef7",
   "metadata": {},
   "source": [
    "Leetcode is an ubiquitous platform used by those in the tech industry to prepare for data structures and algorithms problems that may arise during the interview process. In this notebook, we compare a dataset of originally developed Leetcode solutions (solved by myself over the span of a few years) to a dataset of solutions scraped from a set of leetcode.com forum pages, where one forum page is dedicated to each problem. The analysis is done using a series of techniques from Natural Language Processing. An list of the analyses we perform is below:\n",
    "\n",
    "(Under development)\n",
    "\n",
    "Ultimately, the goal of this project is to discover my strengths and weaknesses (with regards to data structures and algorithms, but also coding more generally) with a data driven methodology and use these to identify personal development areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000023eb",
   "metadata": {},
   "source": [
    "## Part I: Scraping dataset of online solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb628d",
   "metadata": {},
   "source": [
    "Leetcode is a dynamic website built using React. For this reason, lightweight scrapers such as requests + beautifulsoup would not suffice. Instead, we use Selenium to scrape our database of solutions. This is done in 3 parts:\n",
    "\n",
    "1. Scrape a list of all problems and slugs so that we may crawl individual problem pages.\n",
    "2. For each problem, scrape the links of the top 4 pages of posts with the 'python3' tag on the Solutions forum.\n",
    "3. Scrape the text for all post links scraped in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "080cddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "dataset_path = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "77db0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_premium_info(el):\n",
    "        '''\n",
    "        Searches a specific div element given as argument for an orange svg object, which indicates the bounding box for the leetcode premium lock symbol on the left of a problem.\n",
    "        This svg is only present for premium problems.\n",
    "        '''\n",
    "        try:\n",
    "            el.find_element(By.CLASS_NAME, 'text-brand-orange')\n",
    "        except NoSuchElementException:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "def scrape_problems_list(num_pages= 50, problems_per_page=50):\n",
    "    '''\n",
    "    Scrapes master list of problems on leetcode in order to get a dataset of hyperlinks (or equivalently, slugs) to the page dedicated to each individual problem.\n",
    "    '''\n",
    "    problems_dict = defaultdict(list)\n",
    "    for i in range(num_pages):\n",
    "        print(f\"Page: {i+1}\")\n",
    "        source_page = f'https://leetcode.com/problemset/all/?page={i+1}'\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(source_page)\n",
    "        time.sleep(60) if (i and not i % 10) else time.sleep(5) # Allow page to load without implicitly_wait (because we check for premium by checking for exception)\n",
    "        all_rows = driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div/div[2]/div[1]/div[1]/div[6]/div[2]/div/div/div[2]')\n",
    "        j = 0\n",
    "        while j < problems_per_page:\n",
    "            try:\n",
    "                problem_row = all_rows.find_element(By.XPATH, f'./div[{j + 1}]')\n",
    "                elements = problem_row.find_elements(By.CLASS_NAME, 'mx-2')\n",
    "                problems_dict['premium'].append(get_premium_info(elements[0]))\n",
    "                problems_dict['href'].append(elements[1].find_element(By.XPATH, './/div/div/div/div/a').get_attribute('href'))\n",
    "                problems_dict['title'].append(elements[1].find_element(By.XPATH, './/div/div/div/div/a').text)\n",
    "                problems_dict['acceptance'].append(elements[3].find_element(By.XPATH, './/span').text)\n",
    "                problems_dict['difficulty'].append(elements[4].find_element(By.XPATH, './/span').text)\n",
    "            except NoSuchElementException:\n",
    "                print(f'Fewer than {problems_per_page} problems on this page.')\n",
    "            j += 1\n",
    "    return pd.DataFrame(problems_dict)\n",
    "\n",
    "def scrape_solutions_forum(problem_slug, num_pages=4):\n",
    "    '''\n",
    "    Scrapes the solutions forum dedicated to a given problem (specified via problem slug) in order get a dataset of hyperlinks to the top 60 posts (by votes) with the 'python3' tag for that problem.\n",
    "    '''\n",
    "    post_dict = defaultdict(list)\n",
    "    for i in range(1, num_pages + 1):\n",
    "        source_page = f'https://leetcode.com/problems/{problem_slug}/discuss/?currentPage={i}&orderBy=most_votes&query=&tag=python3'\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(source_page)\n",
    "        driver.implicitly_wait(5) # Allow page to load before throwing exception\n",
    "        current_page_posts = driver.find_elements(By.CLASS_NAME, 'topic-item-wrap__2FSZ')\n",
    "        for post in current_page_posts:\n",
    "            href = post.find_element(By.XPATH, './/*[@class=\"topic-item__1Asc\"]/div[1]/div[1]/a').get_attribute('href')\n",
    "            title = post.find_element(By.XPATH, './/*[@class=\"topic-item__1Asc\"]/div[1]/div[1]/a/div/div').text\n",
    "            try:\n",
    "                user = post.find_element(By.XPATH, './/*[@class=\"topic-item__1Asc\"]/div[1]/div[2]/span/span[1]/a').text\n",
    "            except NoSuchElementException:\n",
    "                user = 'deleted_user'\n",
    "            upvotes = post.find_element(By.XPATH, './/*[@class=\"topic-item__1Asc\"]/div[2]/div[1]/div').text\n",
    "            views = post.find_element(By.XPATH, './/*[@class=\"topic-item__1Asc\"]/div[2]/div[2]/div').text\n",
    "            post_dict['slug'].append(problem_slug)\n",
    "            post_dict['title'].append(title)\n",
    "            post_dict['user'].append(user)\n",
    "            post_dict['upvotes'].append(upvotes)\n",
    "            post_dict['views'].append(views)\n",
    "            post_dict['href'].append(href)    \n",
    "        time.sleep(2)\n",
    "    return pd.DataFrame(post_dict)\n",
    "\n",
    "def parse_html_to_python(html_code_block):\n",
    "    lines = html_code_block.split('\\n')\n",
    "    solution_flag = True\n",
    "    python_flag = True\n",
    "    if '<span class=\"hljs-class\">' not in lines[0]:\n",
    "        solution_flag = False\n",
    "    res = [re.sub('</span>', '', re.sub('<span class=\"hljs-[\\s\\S]*?\">', '', line)) for line in lines] # for code formatting / coloring\n",
    "    res = [re.sub('&gt;', '>', re.sub('&lt;', '<', line)) for line in res] # HTML entities\n",
    "    res = '\\n'.join(res)\n",
    "    # Some posts contain solutions for multiple languages. This is a nifty way to check for python solutions, \n",
    "    # since every solution defines a function and no other language requires \"(self,\" in function declarations within a class.\n",
    "    if '(self,' not in res:\n",
    "        python_flag = False\n",
    "    return solution_flag and python_flag, res\n",
    "\n",
    "def scrape_single_post(post_href):\n",
    "    '''\n",
    "    Scrapes a single post to return the text written in the original post. Comments are ignored.\n",
    "    '''\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(post_href)\n",
    "    driver.implicitly_wait(5)\n",
    "    original_post_body = driver.find_elements(By.XPATH, '//*[@id=\"discuss-container\"]/div/div/div[2]/div[1]/div[2]/div[2]/div')[0]\n",
    "    try:\n",
    "        code_blocks = original_post_body.find_elements(By.XPATH, './/pre') # <pre> </pre> used for code formatting\n",
    "    except NoSuchElementException:\n",
    "        pass # No code blocks in the original post\n",
    "    all_solutions_in_post = []\n",
    "    for block in code_blocks:\n",
    "        html = block.find_element(By.XPATH, './/code').get_attribute('innerHTML')\n",
    "        valid_solution, parsed_html = parse_html_to_python(html)\n",
    "        if valid_solution:\n",
    "            all_solutions_in_post.append(parsed_html)\n",
    "    return all_solutions_in_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32898c4",
   "metadata": {},
   "source": [
    "Scrape problems list and preprocess for a list of all problem names, sligs, numbers, acceptance rates, and difficulties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b29b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problems_df = scrape_problems_list() # Uncomment to rescrape data\n",
    "#problems_df.to_csv(f'{dataset_path}/problems_data.csv')\n",
    "problems_df = pd.read_csv(f'{dataset_path}/problems_data.csv', index_col='Unnamed: 0')\n",
    "problems_df['number'] = problems_df['title'].str.split('.').map(lambda x: x[0]).astype('int64')\n",
    "problems_df['title_text'] = problems_df['title'].str.split('.').map(lambda x: x[1]).str[1:]\n",
    "problems_df['slug'] = problems_df['href'].str.split('/').map(lambda x: x[-2])\n",
    "problems_df['acceptance'] = problems_df['acceptance'].str[:-1].astype('float64') / 100. # Convert percentage to decimal\n",
    "\n",
    "problems_df.drop_duplicates(inplace=True) # The featured problem will always be duplicated (once at top of list, once in proper numeric ordering)\n",
    "problems_df.loc[problems_df['premium'] == 0] # Unable to scrape paid problems.\n",
    "problems_df = problems_df[['title_text', 'slug', 'number', 'acceptance', 'difficulty']].sort_values('number').reset_index(drop=True)\n",
    "problems_df.to_csv(f'{dataset_path}/problems_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b852ccf",
   "metadata": {},
   "source": [
    "Loop over problem slugs, saving the first 4 pages of posts (order by most views) with the 'python3' tag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ebea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_df = pd.read_csv(f'{dataset_path}/problems_data_cleaned.csv', index_col='Unnamed: 0')\n",
    "all_posts_hrefs = []\n",
    "for i, slug in enumerate(problems_df['slug']):\n",
    "    df_of_post_hrefs = scrape_solutions_forum(slug)\n",
    "    all_posts_hrefs.append(df_of_post_hrefs)\n",
    "    if i and not i % 10:\n",
    "        temp_save_df = pd.concat(all_posts_hrefs, axis=0)\n",
    "        temp_save_df.to_csv(f'{dataset_path}/post_hrefs/intermediate_post_hrefs_df_{i}.csv')\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        time.sleep(5)\n",
    "all_posts_hrefs_df = pd.concat(all_posts_hrefs, axis=0)\n",
    "all_posts_hrefs.to_csv(f'{dataset_path}/all_posts_hrefs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
